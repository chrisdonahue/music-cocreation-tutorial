{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Co-creation Tutorial Part 1 (Training).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK41YT0yH3vs"
      },
      "source": [
        "# Music Co-creation Tutorial Part 1: Training a generative model of music\n",
        "### [Chris Donahue](https://chrisdonahue.com), [Anna Huang](https://research.google/people/105787/), [Jon Gillick](https://www.jongillick.com/)\n",
        "\n",
        "This is the first part of a two-part tutorial entitled [*Interactive music co-creation with PyTorch and TensorFlow.js*](https://github.com/chrisdonahue/music-cocreation-tutorial/), prepared as part of the ISMIR 2021 tutorial *Designing generative models for interactive co-creation*. This part of the tutorial will demonstrate how to **train a generative model of music in PyTorch**, and **port its weights to TensorFlow.js** format for interaction. The [final result is here](https://chrisdonahue.com/music-cocreation-tutorial)—see our [GitHub repo](https://github.com/chrisdonahue/music-cocreation-tutorial/) for part 2.\n",
        "\n",
        "## Primer on Piano Genie\n",
        "\n",
        "The generative model we will train is called [Piano Genie](https://magenta.tensorflow.org/pianogenie) (Donahue et al. 2019). Piano Genie is a system which maps amateur improvisations on a miniature 8-button keyboard ([video](https://www.youtube.com/watch?v=YRb0XAnUpIk), [demo](https://piano-genie.glitch.me)) into realistic performances on a full 88-key piano.\n",
        "\n",
        "To achieve this, Piano Genie adopts an _autoencoder_ approach. First, an _encoder_ maps professional piano performances into this 8-button space. Then, a _decoder_ attempts to reconstruct the original piano performance from the 8-button version. The entire system is trained end-to-end to minimize the decoder's reconstruction error. At performance time, we replace the encoder with a user improvising on an 8-button controller, and use the pre-trained decoder to generate a corresponding piano performance.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/overview.png\" width=600px/></center>\n",
        "\n",
        "At a low-level, both the encoder and the decoder for Piano Genie are lightweight recurrent neural networks, which are suitable for real-time performance even on mobile CPUs. The discrete bottleneck is achieved using a technique called _integer-quantized autoencoding_ (IQAE), which was also proposed in the Piano Genie paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an946C19rSVJ"
      },
      "source": [
        "#@title **(Step 1)** Parse MIDI piano performances into simple lists of notes\n",
        "\n",
        "# @markdown *Note*: Check this box to rebuild the dataset from scratch.\n",
        "REBUILD_DATASET = False  # @param{type:\"boolean\"}\n",
        "\n",
        "# @markdown To train Piano Genie, we will use a dataset of professional piano performances called [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) (Hawthorne et al. 2019).\n",
        "# @markdown Each performance in this dataset was captured by a Disklavier, a computerized piano which can record human performances in MIDI format, i.e., as timestamped sequences of notes.\n",
        "\n",
        "PIANO_LOWEST_KEY_MIDI_PITCH = 21\n",
        "PIANO_NUM_KEYS = 88\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def download_and_parse_maestro():\n",
        "    # Install pretty_midi\n",
        "    !!pip install pretty_midi\n",
        "    import pretty_midi\n",
        "\n",
        "    # Download MAESTRO dataset (Hawthorne+ 2018)\n",
        "    !!wget -nc https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
        "    !!unzip maestro-v2.0.0-midi.zip\n",
        "\n",
        "    # Parse MAESTRO dataset\n",
        "    dataset = defaultdict(list)\n",
        "    with open(\"maestro-v2.0.0/maestro-v2.0.0.json\", \"r\") as f:\n",
        "        for attrs in tqdm(json.load(f)):\n",
        "            split = attrs[\"split\"]\n",
        "            midi = pretty_midi.PrettyMIDI(\"maestro-v2.0.0/\" + attrs[\"midi_filename\"])\n",
        "            assert len(midi.instruments) == 1\n",
        "            # @markdown Formally, a piano performance is a sequence of notes: $\\mathbf{x} = (x_1, \\ldots, x_N)$, where each $x_i = (t_i, x^e_i, k_i, x^v_i)$, signifying:\n",
        "            notes = [\n",
        "                (\n",
        "                    # @markdown 1. (When the key was pressed) An _onset_ time $t_i \\in \\mathbb{T}$, where $\\mathbb{T} = \\{ t \\in \\mathbb{R} \\mid 0 \\leq t \\leq T \\}$ \n",
        "                    float(n.start),\n",
        "                    # @markdown 2. (When the key was released) An _offset_ time $x^e_i \\in \\mathbb{T}$\n",
        "                    float(n.end),\n",
        "                    # @markdown 3. (Which key was pressed) A _key_ index $k_i \\in \\mathbb{K}$, where $\\mathbb{K} = \\{\\text{A0}, \\ldots, \\text{C8}\\}$ and $|\\mathbb{K}| = 88$\n",
        "                    int(n.pitch - PIANO_LOWEST_KEY_MIDI_PITCH),\n",
        "                    # @markdown 4. (How hard the key was pressed) A _velocity_ $x^v_i \\in \\mathbb{V}$, where $\\mathbb{V} = \\{1, \\ldots, 127\\}$\n",
        "                    int(n.velocity),\n",
        "                )\n",
        "                for n in midi.instruments[0].notes\n",
        "            ]\n",
        "\n",
        "            # This list is in sorted order of onset time, i.e., $x_{i-1}^s \\leq x_i^s ~\\forall~i \\in \\{2, \\ldots, N\\}$.\n",
        "            notes = sorted(notes, key=lambda n: (n[0], n[2]))\n",
        "            assert all(\n",
        "                [\n",
        "                    all(\n",
        "                        [\n",
        "                            # Start times should be non-negative\n",
        "                            n[0] >= 0,\n",
        "                            # Note durations should be strictly positive, i.e., $x_i^s < x_i^e$\n",
        "                            n[0] < n[1],\n",
        "                            # Key index should be in range of the piano\n",
        "                            0 <= n[2] and n[2] < PIANO_NUM_KEYS,\n",
        "                            # Velocity should be valid\n",
        "                            1 <= n[3] and n[3] < 128,\n",
        "                        ]\n",
        "                    )\n",
        "                    for n in notes\n",
        "                ]\n",
        "            )\n",
        "            dataset[split].append(notes)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "if REBUILD_DATASET:\n",
        "    DATASET = download_and_parse_maestro()\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"w\") as f:\n",
        "        f.write(json.dumps(DATASET).encode(\"utf-8\"))\n",
        "else:\n",
        "    !!wget -nc https://github.com/chrisdonahue/music-cocreation-tutorial/raw/main/part-1-py-training/data/maestro-v2.0.0-simple.json.gz\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"rb\") as f:\n",
        "        DATASET = json.load(f)\n",
        "\n",
        "print([(s, len(DATASET[s])) for s in [\"train\", \"validation\", \"test\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQ9PAvMCwd2"
      },
      "source": [
        "# @title **(Step 2)** Define Piano Genie autoencoder\n",
        "\n",
        "# @markdown Our intended interaction for Piano Genie is to have users perform on a miniature 8-button keyboard.\n",
        "# @markdown Similarly to how we formalized professional piano performances, we will represent these \"button performances\" as sequences of \"notes\", where we replace piano keys $k_i$ with buttons $b_i$, and we remove velocity since our controllers are not velocity-sensitive. So, to summarize, piano performances $\\mathbf{x}$ and button performances $\\mathbf{c}$ are defined as follows:\n",
        "\n",
        "# @markdown - $\\mathbf{x} = (x_1, \\ldots, x_N)$, where $x_i = (t_i \\in \\mathbb{T}, x^e_i \\in \\mathbb{T}, k_i \\in \\mathbb{K}, x^v_i \\in \\mathbb{V})$, i.e., (onsets, offsets, keys, velocities)\n",
        "\n",
        "# @markdown - $\\mathbf{c} = (c_1, \\ldots, c_M)$, where $c_i = (c^s_i \\in \\mathbb{T}, c^e_i \\in \\mathbb{T}, b_i \\in \\mathbb{B})$, i.e., (onsets, offsets, buttons), and $\\mathbb{B} = \\{ \\color{#EE2B29}\\blacksquare, \\color{#ff9800}\\blacksquare, \\color{#ffff00}\\blacksquare, \\color{#c6ff00}\\blacksquare, \\color{#00e5ff}\\blacksquare, \\color{#2979ff}\\blacksquare, \\color{#651fff}\\blacksquare, \\color{#d500f9}\\blacksquare \\}$\n",
        "\n",
        "# @markdown To map button performances into piano performances, we will train a generative model $P(\\mathbf{x} \\mid \\mathbf{c})$.\n",
        "# @markdown In practice, we will factorize this joint distribution over note sequences $\\mathbf{x}$ into the product of conditional probabilities of individual notes: $P(\\mathbf{x} \\mid \\mathbf{c}) = \\prod_{i=1}^{N} P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$. \n",
        "\n",
        "# @markdown Hence, our **overall goal is to learn** $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$, \n",
        "# @markdown which we will **approximate by modeling**:\n",
        "\n",
        "# @markdown <center>$P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$.</center>\n",
        "\n",
        "# @markdown We arrived at this approximation by working through constraints imposed by the interaction (details at the end).\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# @markdown #### **Decoder**\n",
        "\n",
        "# @markdown <center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/decoder.png\" width=600px/></center>\n",
        "# @markdown <center><b>Piano Genie decoder processing $N=4$ notes</b></center>\n",
        "\n",
        "# @markdown The approximation $P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$ constitutes the decoder of Piano Genie, which we will parameterize using an RNN.\n",
        "# @markdown This is the portion of the model that users will interact with. \n",
        "# @markdown To achieve our intended real-time interaction, we will compute and sample from this RNN at the instant the user presses a button, passing as input the key from the previous timestep, the current time, the button the user pressed, and a vector which summarizes the ongoing history.\n",
        "\n",
        "# @markdown Formally, the decoder is a function:\n",
        "# @markdown $D_{\\theta}: k_{i-1}, t_i, b_i, \\mathbf{h}_{i-1} \\mapsto \\mathbf{\\hat{k}}_i, \\mathbf{h}_i$, where:\n",
        "\n",
        "# @markdown - $k_0$ is a special start-of-sequence token $<\\text{S}>$\n",
        "\n",
        "# @markdown - $\\mathbf{h}_i$ is a vector summarizing timesteps $1, \\ldots, i$\n",
        "\n",
        "# @markdown - $\\mathbf{h}_0$ is some initial value (zeros) for that vector\n",
        "\n",
        "# @markdown - $\\mathbf{\\hat{k}}_i \\in \\mathbb{R}^{88}$ are the output logits for timestep $i$\n",
        "\n",
        "SOS = PIANO_NUM_KEYS\n",
        "\n",
        "class PianoGenieDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(PIANO_NUM_KEYS + 3, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim, 88)\n",
        "    \n",
        "    def init_hidden(self, batch_size, device=None):\n",
        "        h = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim)\n",
        "        c = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim)\n",
        "        if device is not None:\n",
        "            h = h.to(device)\n",
        "            c = c.to(device)\n",
        "        return (h, c)\n",
        "\n",
        "    def forward(self, k, t, b, h_0=None):\n",
        "        # Prepend <S> token to shift k_i to k_{i-1}\n",
        "        k_m1 = torch.cat([torch.full_like(k[:, :1], SOS), k[:, :-1]], dim=1)\n",
        "\n",
        "        # Encode input\n",
        "        inputs = [\n",
        "            F.one_hot(k_m1, PIANO_NUM_KEYS + 1),\n",
        "            t.unsqueeze(dim=2),\n",
        "            b.unsqueeze(dim=2),\n",
        "        ]\n",
        "        x = torch.cat(inputs, dim=2)\n",
        "\n",
        "        # Project encoded inputs\n",
        "        x = self.input(x)\n",
        "\n",
        "        # Run RNN\n",
        "        if h_0 is None:\n",
        "            h_0 = self.init_hidden(k.shape[0], device=k.device)\n",
        "        x, h_N = self.lstm(x, h_0)\n",
        "\n",
        "        # Compute logits\n",
        "        hat_k = self.output(x)\n",
        "\n",
        "        return hat_k, h_N\n",
        "\n",
        "\n",
        "# @markdown #### **Encoder**\n",
        "\n",
        "# @markdown <center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/encoder.png\" width=600px/></center>\n",
        "# @markdown <center><b>Piano Genie encoder processing $N=4$ notes</b></center>\n",
        "\n",
        "# @markdown Because we lack examples of human button performances, we use an encoder to automatically learn to map piano performances into synthetic button performances.\n",
        "# @markdown Our encoder is also an RNN, though it is bidirectional unlike the decoder. \n",
        "# @markdown This allows it to observe the entire piano performance before compressing it into buttons.\n",
        "\n",
        "# @markdown Formally, the encoder is a function: $E_{\\varphi} : k_i, t_i, \\mathbf{h^f}_{i-1}, \\mathbf{h^b}_{N-i} \\mapsto b_i, \\mathbf{h^f}_i, \\mathbf{h^b}_{N - i + 1}$, where $\\mathbf{h^f}$ / $\\mathbf{h^b}$ are summary vectors in the forwards and backwards directions respectively, and $\\mathbf{h^f}_0$ / $\\mathbf{h^b}_0$ are their respective initial values.\n",
        "\n",
        "class PianoGenieEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(PIANO_NUM_KEYS + 1, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim * 2, 1)\n",
        "\n",
        "    def forward(self, k, t):\n",
        "        inputs = [\n",
        "            F.one_hot(k, PIANO_NUM_KEYS),\n",
        "            t.unsqueeze(dim=2),\n",
        "        ]\n",
        "        x = self.input(torch.cat(inputs, dim=2))\n",
        "        # NOTE: PyTorch uses zeros automatically if h is None\n",
        "        x, _ = self.lstm(x, None)\n",
        "        x = self.output(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "# @markdown #### **Quantizing encoder output to discrete buttons**\n",
        "\n",
        "# @markdown <center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/quantization.png\" width=600px/></center>\n",
        "# @markdown <center><b>Quantizing continuous encoder output (grey line) to eight discrete values (colorful line segments)</b></center>\n",
        "\n",
        "# @markdown You may have noticed that the encoder outputs a real-valued scalar (let's call it $e_i \\in \\mathbb{R}$) at each timestep, but our goal is to output one of eight discrete buttons, i.e., $b_i \\in \\mathbb{B}$. \n",
        "# @markdown To achieve this, we will quantize this real-valued scalar as the centroid of the nearest of eight bins between $[-1, 1]$ (see figure above):\n",
        "\n",
        "# @markdown <center>$b_i = 2 \\cdot \\frac{\\tilde{b}_i - 1}{B - 1} - 1$, where $\\tilde{b}_i = \\text{round} \\left( 1 + (B - 1) \\cdot \\min \\left( \\max \\left( \\frac{e_i  + 1}{2}, 0 \\right), 1 \\right) \\right)$</center>\n",
        "\n",
        "class IntegerQuantizer(nn.Module):\n",
        "    def __init__(self, num_bins):\n",
        "        super().__init__()\n",
        "        self.num_bins = num_bins\n",
        "\n",
        "    def real_to_discrete(self, x, eps=1e-6):\n",
        "        x = (x + 1) / 2\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        x *= self.num_bins - 1\n",
        "        x = (torch.round(x) + eps).long()\n",
        "        return x\n",
        "\n",
        "    def discrete_to_real(self, x):\n",
        "        x = x.float()\n",
        "        x /= self.num_bins - 1\n",
        "        x = (x * 2) - 1\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantize and compute delta (used for straight-through estimator)\n",
        "        with torch.no_grad():\n",
        "            x_disc = self.real_to_discrete(x)\n",
        "            x_quant = self.discrete_to_real(x_disc)\n",
        "            x_quant_delta = x_quant - x\n",
        "\n",
        "        # @markdown In the backwards pass, we will use the straight-through estimator (Bengio et al. 2013), i.e., pretend that this discretization did not happen when computing gradients.\n",
        "        # Quantize w/ straight-through estimator\n",
        "        x = x + x_quant_delta\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# @markdown #### **Defining the autoencoder**\n",
        "\n",
        "# @markdown Finally, the Piano Genie autoencoder is simply the composition of the encoder, quantizer, and decoder.\n",
        "\n",
        "class PianoGenieAutoencoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.enc = PianoGenieEncoder(\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "        self.quant = IntegerQuantizer(cfg[\"num_buttons\"])\n",
        "        self.dec = PianoGenieDecoder(\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "\n",
        "    def forward(self, k, t):\n",
        "        e = self.enc(k, t)\n",
        "        b = self.quant(e)\n",
        "        hat_k, _ = self.dec(k, t, b)\n",
        "        return hat_k, e\n",
        "\n",
        "\n",
        "# @markdown #### **Approximating $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$**\n",
        "\n",
        "# @markdown This section walks through how we designed an approximation to $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$ which would be appropriate for our intended interaction. You probably don't need to understand this, but some may find it helpful as an illustration of how to design a generative model around constraints imposed by interaction.\n",
        "\n",
        "# @markdown First, we expand the terms:\n",
        "\n",
        "# @markdown <center>$P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c}) = P(t_i, x^e_i, k_i, x^v_i \\mid \\mathbf{t}_{<i}, \\mathbf{x^e}_{<i}, \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{c^s}, \\mathbf{c^e}, \\mathbf{b})$</center>\n",
        "\n",
        "# @markdown We think it might be intuitive for the miniature piano to behave like a real piano: pressing a button causes a note to sound, which is held until released. Hence, $N = M$, $t_i = c^s_i$, and $x^e_i = c^e_i$, so we can remove some redundant terms:\n",
        "\n",
        "# @markdown <center>$= P(k_i, x^v_i \\mid \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{t}, \\mathbf{c^e}, \\mathbf{b})$</center>\n",
        "\n",
        "# @markdown Because we want this interaction to be real-time, we must remove any term that might not be available at $t_i$, which includes future onsets $\\mathbf{t}_{>i}$, future buttons $\\mathbf{b}_{>i}$, and all offsets $\\mathbf{c^e}$, since notes can be held indefinitely:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, x^v_i \\mid \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>\n",
        "\n",
        "# @markdown Finally, we anticipate that it will be frustrating for users if the model predicts dynamics on their behalf, so we remove velocity terms $\\mathbf{x^v}$:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_fReq-uCfoy"
      },
      "source": [
        "# @title **(Step 3)** Train Piano Genie\n",
        "\n",
        "# @markdown *Note*: Check this box to log training curves to [Weights & Biases](https://wandb.ai/) (which will prompt you to log in).\n",
        "USE_WANDB = False  # @param{type:\"boolean\"}\n",
        "\n",
        "# @markdown Now that we've defined the autoencoder, we need to train it.\n",
        "# @markdown We will train the entire autoencoder end-to-end to minimize the reconstruction loss of the decoder.\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{recons}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CrossEntropy}(\\text{Softmax}(\\mathbf{\\hat{k}}_i), k_i)$</center>\n",
        "\n",
        "# @markdown This loss alone does not encourage the encoder to produce button sequences with any particular structure, so the behavior of the decoder will likely be fairly unpredictable at interaction time.\n",
        "# @markdown We think it might be intuitive to users if the decoder respected the _contour_ of their performance, i.e., if higher buttons produced higher notes and lower buttons produced lower notes.\n",
        "# @markdown Hence, we include a loss term which encourages the encoder to produces button sequences which align with the contour of the piano key sequences.\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{contour}} = \\frac{1}{N - 1} \\sum_{i=2}^{N} \\max (0, 1 - (k_i - k_{i-1}) \\cdot (e_i - e_{i-1}))^2$</center>\n",
        "\n",
        "# @markdown Finally, we find empirically that the encoder often outputs values outside of the $[-1, 1]$ range used for discretization. \n",
        "# @markdown Hence, we add a loss term which explicitly encourages this behavior\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{margin}} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, |e_i| - 1)^2$</center>\n",
        "\n",
        "# @markdown Thus, our final loss function is:\n",
        "# @markdown <center>$\\mathcal{L} = \\mathcal{L}_{\\text{recons}} + \\mathcal{L}_{\\text{contour}} + \\mathcal{L}_{\\text{margin}}$</center>\n",
        "\n",
        "\n",
        "CFG = {\n",
        "    \"seed\": 0,\n",
        "    # Number of buttons in interface\n",
        "    \"num_buttons\": 8,\n",
        "    # Onset delta times will be clipped to this maximum\n",
        "    \"data_delta_time_max\": 1.0,\n",
        "    # Max time stretch for data augmentation (+- 5%)\n",
        "    \"data_augment_time_stretch_max\": 0.05,\n",
        "    # Max transposition for data augmentation (+- tritone)\n",
        "    \"data_augment_transpose_max\": 6,\n",
        "    # RNN dimensionality\n",
        "    \"model_rnn_dim\": 128,\n",
        "    # RNN num layers\n",
        "    \"model_rnn_num_layers\": 2,\n",
        "    # Training hyperparameters\n",
        "    \"batch_size\": 32,\n",
        "    \"seq_len\": 128,\n",
        "    \"lr\": 3e-4,\n",
        "    \"loss_margin_multiplier\": 1.0,\n",
        "    \"loss_contour_multiplier\": 1.0,\n",
        "    \"summarize_frequency\": 128,\n",
        "    \"eval_frequency\": 128,\n",
        "    \"max_num_steps\": 50000\n",
        "}\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "    except ModuleNotFoundError:\n",
        "        !!pip install wandb\n",
        "        import wandb\n",
        "\n",
        "# Init\n",
        "run_dir = pathlib.Path(\"piano_genie\")\n",
        "run_dir.mkdir(exist_ok=True)\n",
        "with open(pathlib.Path(run_dir, \"cfg.json\"), \"w\") as f:\n",
        "    f.write(json.dumps(CFG, indent=2))\n",
        "if USE_WANDB:\n",
        "    wandb.init(project=\"music-cocreation-tutorial\", config=CFG, reinit=True)\n",
        "\n",
        "# Set seed\n",
        "if CFG[\"seed\"] is not None:\n",
        "    random.seed(CFG[\"seed\"])\n",
        "    np.random.seed(CFG[\"seed\"])\n",
        "    torch.manual_seed(CFG[\"seed\"])\n",
        "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
        "\n",
        "# Create model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.train()\n",
        "model.to(device)\n",
        "print(\"-\" * 80)\n",
        "for n, p in model.named_parameters():\n",
        "    print(f\"{n}, {p.shape}\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "\n",
        "# Subsamples performances to create a minibatch\n",
        "def performances_to_batch(performances, device, train=True):\n",
        "    batch_k = []\n",
        "    batch_t = []\n",
        "    for p in performances:\n",
        "        # Subsample seq_len notes from performance\n",
        "        assert len(p) >= CFG[\"seq_len\"]\n",
        "        if train:\n",
        "            subsample_offset = random.randrange(0, len(p) - CFG[\"seq_len\"])\n",
        "        else:\n",
        "            subsample_offset = 0\n",
        "        subsample = p[subsample_offset : subsample_offset + CFG[\"seq_len\"]]\n",
        "        assert len(subsample) == CFG[\"seq_len\"]\n",
        "\n",
        "        # Data augmentation\n",
        "        if train:\n",
        "            stretch_factor = random.random() * CFG[\"data_augment_time_stretch_max\"] * 2\n",
        "            stretch_factor += 1 - CFG[\"data_augment_time_stretch_max\"]\n",
        "            transposition_factor = random.randint(\n",
        "                -CFG[\"data_augment_transpose_max\"], CFG[\"data_augment_transpose_max\"]\n",
        "            )\n",
        "            subsample = [\n",
        "                (\n",
        "                    n[0] * stretch_factor,\n",
        "                    n[1] * stretch_factor,\n",
        "                    max(0, min(n[2] + transposition_factor, PIANO_NUM_KEYS - 1)),\n",
        "                    n[3],\n",
        "                )\n",
        "                for n in subsample\n",
        "            ]\n",
        "        \n",
        "        # Key features\n",
        "        batch_k.append([n[2] for n in subsample])\n",
        "\n",
        "        # Onset features\n",
        "        # NOTE: For stability, we pass delta time to Piano Genie instead of time.\n",
        "        t = np.diff([n[0] for n in subsample])\n",
        "        t = np.concatenate([[1e8], t])\n",
        "        t = np.clip(t, 0, CFG[\"data_delta_time_max\"])\n",
        "        batch_t.append(t)\n",
        "\n",
        "    return (torch.tensor(batch_k).long(), torch.tensor(batch_t).float())\n",
        "\n",
        "\n",
        "# Train\n",
        "step = 0\n",
        "best_eval_loss = float(\"inf\")\n",
        "while CFG[\"max_num_steps\"] is None or step < CFG[\"max_num_steps\"]:\n",
        "    if step % CFG[\"eval_frequency\"] == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_losses = []\n",
        "            for i in range(0, len(DATASET[\"validation\"]), CFG[\"batch_size\"]):\n",
        "                eval_batch = performances_to_batch(\n",
        "                    DATASET[\"validation\"][i : i + CFG[\"batch_size\"]],\n",
        "                    device,\n",
        "                    train=False,\n",
        "                )\n",
        "                eval_k, eval_t = tuple(t.to(device) for t in eval_batch)\n",
        "                eval_hat_k, _ = model(eval_k, eval_t)\n",
        "                eval_loss = F.cross_entropy(\n",
        "                    eval_hat_k.view(-1, PIANO_NUM_KEYS),\n",
        "                    eval_k.view(-1),\n",
        "                    reduction=\"none\",\n",
        "                )\n",
        "                eval_losses.extend(eval_loss.cpu().numpy().tolist())\n",
        "\n",
        "            eval_loss = np.mean(eval_losses)\n",
        "            if eval_loss < best_eval_loss:\n",
        "                torch.save(model.state_dict(), pathlib.Path(run_dir, \"model.pt\"))\n",
        "                best_eval_loss = eval_loss\n",
        "\n",
        "        eval_metrics = {\"eval_loss_recons\": eval_loss}\n",
        "        if USE_WANDB:\n",
        "            wandb.log(eval_metrics, step=step)\n",
        "        print(step, \"eval\", eval_metrics)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Create minibatch\n",
        "    batch = performances_to_batch(\n",
        "        random.sample(DATASET[\"train\"], CFG[\"batch_size\"]), device, train=True\n",
        "    )\n",
        "    k, t = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Run model\n",
        "    optimizer.zero_grad()\n",
        "    k_hat, e = model(k, t)\n",
        "\n",
        "    # Compute losses and update params\n",
        "    loss_recons = F.cross_entropy(k_hat.view(-1, PIANO_NUM_KEYS), k.view(-1))\n",
        "    loss_margin = torch.square(\n",
        "        torch.maximum(torch.abs(e) - 1, torch.zeros_like(e))\n",
        "    ).mean()\n",
        "    loss_contour = torch.square(\n",
        "        torch.maximum(\n",
        "            1 - torch.diff(k, dim=1) * torch.diff(e, dim=1),\n",
        "            torch.zeros_like(e[:, 1:]),\n",
        "        )\n",
        "    ).mean()\n",
        "    loss = loss_recons\n",
        "    if CFG[\"loss_margin_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_margin_multiplier\"] * loss_margin\n",
        "    if CFG[\"loss_contour_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_contour_multiplier\"] * loss_contour\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "    if step % CFG[\"summarize_frequency\"] == 0:\n",
        "        metrics = {\n",
        "            \"loss_recons\": loss_recons.item(),\n",
        "            \"loss_margin\": loss_margin.item(),\n",
        "            \"loss_contour\": loss_contour.item(),\n",
        "            \"loss\": loss.item(),\n",
        "        }\n",
        "        if USE_WANDB:\n",
        "            wandb.log(metrics, step=step)\n",
        "        print(step, \"train\", metrics)\n",
        "\n",
        "# Download the trained model so we don't lose it!\n",
        "from google.colab import files\n",
        "\n",
        "files.download('piano_genie/model.pt')\n",
        "files.download('piano_genie/cfg.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj49WCPSAlyf"
      },
      "source": [
        "# @title **(Step 4)** Port trained decoder parameters to Tensorflow.js format\n",
        "\n",
        "# @markdown In this step, we will use the TensorFlow.js Python library to export our model's parameters in a binary format, to be loaded later by the JavaScript client.\n",
        "\n",
        "!!pip install tensorflowjs\n",
        "\n",
        "from tensorflowjs.write_weights import write_weights\n",
        "\n",
        "# Load saved model dict\n",
        "d = torch.load(\"piano_genie/model.pt\", map_location=torch.device(\"cpu\"))\n",
        "d = {k: v.numpy() for k, v in d.items()}\n",
        "\n",
        "# Convert to tensorflow-js format\n",
        "pathlib.Path(\"piano_genie/dec_tfjs\").mkdir(exist_ok=True)\n",
        "write_weights(\n",
        "    [[{\"name\": k, \"data\": v} for k, v in d.items() if k.startswith(\"dec\")]],\n",
        "    \"piano_genie/dec_tfjs\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBuHu2Hohc5f"
      },
      "source": [
        "# @title **(Step 5)** Create test fixtures check correctness of JavaScript port\n",
        "\n",
        "# @markdown Finally, we will serialize a sequence of inputs to and outputs from our trained model to create a test case for our JavaScript reimplementation.\n",
        "# @markdown This is critically important—I have ported many models from Python to JavaScript and have yet to get it right on the first try.\n",
        "# @markdown Porting models from PyTorch to TensorFlow.js is additionally tricky because parameters of the same shape are often used differently by the two APIs.\n",
        "\n",
        "# Restore model from saved checkpoint\n",
        "device = torch.device(\"cpu\")\n",
        "with open(\"piano_genie/cfg.json\", \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "model = PianoGenieAutoencoder(cfg)\n",
        "model.load_state_dict(torch.load(\"piano_genie/model.pt\", map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Serialize a batch of inputs/outputs as JSON\n",
        "with torch.no_grad():\n",
        "    ground_truth_keys, input_dts = performances_to_batch(\n",
        "        [DATASET[\"validation\"][0]], device, train=False\n",
        "    )\n",
        "    output_logits, input_buttons = model(ground_truth_keys, input_dts)\n",
        "    input_buttons = model.quant.real_to_discrete(input_buttons)\n",
        "\n",
        "    input_dts = input_dts[0].cpu().numpy().tolist()\n",
        "    ground_truth_keys = ground_truth_keys[0].cpu().numpy().tolist()\n",
        "    input_keys = [PIANO_NUM_KEYS] + ground_truth_keys[:-1]\n",
        "    input_buttons = input_buttons[0].cpu().numpy().tolist()\n",
        "    output_logits = output_logits[0].cpu().numpy().tolist()\n",
        "\n",
        "    fixtures = {\n",
        "        n: eval(n)\n",
        "        for n in [\"input_dts\", \"input_keys\", \"input_buttons\", \"output_logits\"]\n",
        "    }\n",
        "    with open(pathlib.Path(\"piano_genie\", \"fixtures.json\"), \"w\") as f:\n",
        "        f.write(json.dumps(fixtures))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}